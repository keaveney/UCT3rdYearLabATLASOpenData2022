{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='hzz.png' width=\"570\"></td><td><img src='ATLAS-figaux_01.png' width=\"800\"></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  How much Higgs? - Intro\n",
    "- **Overview**\n",
    "    - The notebook is based on statistical analysis of open data and simulation from the ATLAS experiment at CERN.  Even if the physics context of this data is not entirely clear to you, it should not be a problem as understanding anything beyond the next paragraph is not neccessary.\n",
    "\n",
    "- **The physics in a nutshell**\n",
    "    - The Higgs boson was theorised in the 1960s as the fundamental particle that is responsible for giving mass to other fundamental particles. The problem is, the Higgs boson is extremely difficult to produce in lab conditions. Hence the Large Hadron Collider had to be built at CERN in order to provide enough high energy particle collisions to produce a significant Higgs bosons such that we could confirm the existence of this elusive boson. In 2012, the ATLAS and CMS experiments at the LHC announced that they had finally discovered the Higgs boson and the Nobel Prize in physics was subsequently awarded to Peter Higgs and Francois Englert for the boson's prediction. One of the easiest ways to find a Higgs boson in the ATLAS data is to look for collisions (often called 'events') containing 4 *leptons* which can be either electrons or muons. Muons are the slightly more massive cousin of the electron. This is because the Higgs boson can decay into two Z bosons, which in turn each decay into two leptons, leading to the four lepton signal. This process is depicted in the left hand diagram above where (reading left to right) two gluons collide to form a Higgs boson, which subsequently decays to two Z bosons and then four leptons. The right had image above shows a computerised illustration of **real** collision from the ATLAS data that contained four muons. In this notebook we will search through the ATLAS data for these events and compare what we find to predictions. \n",
    "\n",
    "- **Technical details**\n",
    "    - The notebook contains code that performs a simple analysis of the data and simulation. All code is Python 3 and utilised quite a few python package which must be installed before running the notebook. All required packages are listed in the 'requirements.txt' file.  The code is interspersed with exercises where you are either asked to discuss the analysis in the previous cell and/or extend the existing analysis. Specifically, we will look at collisions contains 4 leptons (electrons or muons) and use a histogram of the masses of the 4 lepton system ($m_{llll}$) to play with some statistical ideas. In the last few excercises you are required to perform some analyses with less guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import some crucial python modules that will allow us to analyse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import uproot_methods.classes.TLorentzVector as LVepm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import infofile\n",
    "import numpy as np\n",
    "import mplhep as hep\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the following helper function to help use to calibrate the simualtion, we dont need to worry about what it actually does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xsec_weight(sample):\n",
    "    info = infofile.infos[sample] # open infofile\n",
    "    xsec_weight = (lumi*1000*info[\"xsec\"])/(info[\"sumw\"]*info[\"red_eff\"]) #*1000 to go from fb-1 to pb-1\n",
    "    return xsec_weight # return cross-section weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are just defining the names of the files from where we will read the data and simulation. All data and MC samples should be downloaded to the local directory from https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/4lep/. The last file is the real collision data from ATLAS, the others are simulations of the physics processes we expect to have aoccured in the collisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "\"mc_361106.Zee.4lep.root\",\n",
    "\"mc_361107.Zmumu.4lep.root\",\n",
    "\"mc_410000.ttbar_lep.4lep.root\",\n",
    "\"mc_363490.llll.4lep.root\",\n",
    "\"mc_363492.llvv.4lep.root\",\n",
    "\"mc_363356.ZqqZll.4lep.root\",    \n",
    "\"mc_345060.ggH125_ZZ4lep.4lep.root\",\n",
    "\"mc_341964.WH125_ZZ4lep.4lep.root\",\n",
    "\"mc_344235.VBFH125_ZZ4lep.4lep.root\",\n",
    "\"mc_341947.ZH125_ZZ4lep.4lep.root\",\n",
    "\"data.4lep.root\"  \n",
    "]\n",
    "\n",
    "\n",
    "#NOTE - NONE OF THE REST OF THIS NOTEBOOK WILL RUN UNTIL YOU DOWNLOAD THE INPUT DATA AND SIMULATION FILES FROM HERE.\n",
    "# https://drive.google.com/drive/folders/1iqUqc277CDBXsfUks0Z3i9EzhH0C5_Fp?usp=sharing\n",
    "# You will need to be logged in to google with your UCT account to gain access\n",
    "#PLACE ALL FILES IN THE SAME DIRECTORY AS THIS NOTEBOOK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to select out the most interesting collisions from the data and simulation so that we can as clear as possible signal of Higgs boson production. The exact details of the cuts are not very important. In summary, we select collision in which four leptons (electrons or muons) have been detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File has been successfully opened!\n",
      "<class 'awkward.array.jagged.JaggedArray'>\n",
      "[[63834.66 21530.459 14459.594 7744.2534] [45412.223 45201.867 12077.552 8478.665] [634492.4 45130.008 43174.137 11346.41] ... [45759.707 39300.016 7871.5405 7596.5938] [42553.797 37560.52 9861.585 7430.7646] [62782.72 34426.656 33521.055 17723.377]]\n",
      "File has been successfully opened!\n",
      "<class 'awkward.array.jagged.JaggedArray'>\n",
      "[[48533.914 34416.24 14783.072 7616.2803] [46588.836 40959.285 14568.773 9568.289] [44772.13 44686.766 13851.251 8006.872] ... [61639.0 34548.258 13042.535 7182.4575] [52856.918 31513.613 12825.594 7052.375] [48602.227 45643.58 7821.18 7570.761]]\n",
      "File has been successfully opened!\n",
      "<class 'awkward.array.jagged.JaggedArray'>\n",
      "[[72882.97 38832.246 12576.012 7013.978] [132460.03 51814.49 7977.6675 7039.7715] [74547.516 50520.613 12462.047 7799.5933] ... [47612.863 44278.62 13041.843 7166.7607] [65106.645 60042.895 13987.725 8105.575] [183709.83 10386.254 10364.022 8407.727]]\n",
      "File has been successfully opened!\n",
      "<class 'awkward.array.jagged.JaggedArray'>\n",
      "[[64853.69 61677.957 48666.44 44360.594] [55921.848 41498.75 18562.252 11200.327] [84155.97 78250.46 56973.09 56136.598] ... [119936.56 102871.875 54995.215 20621.857] [32009.953 31902.018 20184.078 14703.7] [75909.305 49681.914 11938.686 7546.0493]]\n",
      "File has been successfully opened!\n",
      "<class 'awkward.array.jagged.JaggedArray'>\n",
      "[[87986.39 58304.336 11205.803 8963.188] [88454.73 65927.055 8487.218 7593.3076] [71155.805 44978.258 10636.307 8559.743 8436.659] ... [78984.63 18137.117 14828.462 7624.121] [114068.836 39445.566 7946.8394 7556.637] [77654.164 12202.973 11480.332 8282.395]]\n",
      "File has been successfully opened!\n",
      "<class 'awkward.array.jagged.JaggedArray'>\n",
      "[[52194.367 36277.875 15191.737 7723.364] [52954.02 36192.754 13200.417 8462.7295] [42757.03 28084.367 12596.537 11847.583] ... [70978.78 40045.613 8857.2295 7203.1387] [62339.02 24528.666 14677.561 13323.394] [53147.38 42090.793 12108.885 7115.926]]\n",
      "File has been successfully opened!\n",
      "<class 'awkward.array.jagged.JaggedArray'>\n",
      "[[51905.457 41248.57 16397.67 7471.2275] [41430.645 40307.168 16133.789 7481.8574] [33646.71 27313.271 20035.95 16472.64] ... [63284.21 22707.84 15635.994 14873.25] [52538.805 40321.457 25766.85 19381.92] [58730.094 35112.83 12265.971 11089.536]]\n",
      "File has been successfully opened!\n",
      "<class 'awkward.array.jagged.JaggedArray'>\n",
      "[[80880.266 72779.7 42110.105 13561.51] [57186.625 40846.812 35407.5 10427.198] [92854.95 25943.02 10871.199 8797.154] ... [200267.84 126925.01 47343.51 14729.97] [34381.098 31525.55 12999.437 8076.2725] [43109.094 39135.535 21054.283 12674.296]]\n",
      "File has been successfully opened!\n",
      "<class 'awkward.array.jagged.JaggedArray'>\n",
      "[[98138.2 21995.48 20338.168 8456.644] [52162.59 43042.38 24589.73 16738.777] [50315.68 19619.604 15251.94 12991.47] ... [43349.797 38485.945 20618.387 9454.492] [51612.383 37433.887 27651.498 7754.202] [66100.29 36531.71 19537.352 9650.627]]\n",
      "File has been successfully opened!\n",
      "<class 'awkward.array.jagged.JaggedArray'>\n",
      "[[63923.805 42847.305 15393.414 13567.796] [83088.76 33954.992 33166.934 26187.252] [76425.484 36745.63 33675.18 17161.502] ... [67050.06 43340.984 36336.61 8388.606] [71643.92 27985.783 20745.455 9867.459] [53283.18 27286.281 24619.783 11846.508 7304.601]]\n",
      "File has been successfully opened!\n",
      "<class 'awkward.array.jagged.JaggedArray'>\n",
      "[[49138.344 43451.65 43018.27 27352.213] [175558.73 138846.67 106696.53 79776.73] [65284.066 37290.766 12426.926 8804.564] ... [58532.07 40881.12 10991.208 8818.951] [88545.625 50751.836 25529.92 22951.45] [198675.45 79746.0 45047.184 28545.074]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#------------DEFINING SOME VARIABLES AND OBJECTS FOR THE ANALYSIS----------------------------------------------------\n",
    "\n",
    "lumi = 10#fb^-1\n",
    "nBins = 34\n",
    "\n",
    "minMass = 80\n",
    "maxMass = 250\n",
    "\n",
    "bins_ar = np.linspace(minMass, maxMass, num=(nBins+1))\n",
    "mc_hist_list = []\n",
    "sample_names = []\n",
    "\n",
    "f = plt.figure()\n",
    "\n",
    "H_125 = np.zeros([nBins])\n",
    "H_bkg = np.zeros([nBins])\n",
    "\n",
    "btagWP77 = 0.6459\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for file in files: #looping over data and simulation files\n",
    "    sample_name = file.split(\".\")[1] \n",
    "    sample_names.append(sample_name)\n",
    "    tree = uproot.open(file)[\"mini\"]\n",
    "\n",
    "    mcWeight, SumWeights, XSection, trigM, trigE, scaleFactor_PILEUP, scaleFactor_ELE, scaleFactor_MUON,scaleFactor_LepTRIGGER, lep_type, lep_pt, lep_eta, lep_phi, lep_E, lep_charge, lep_etcone20, lep_ptcone30, jet_n, jet_pt, jet_eta, jet_phi, jet_E, jet_MV2c10 = tree.arrays([\"mcWeight\", \"SumWeights\", \"XSection\",\"trigM\", \"trigE\",\"scaleFactor_PILEUP\", \"scaleFactor_ELE\", \"scaleFactor_MUON\",\"scaleFactor_LepTRIGGER\",\"lep_type\",\"lep_pt\", \"lep_eta\",\"lep_phi\", \"lep_E\", \"lep_charge\", \"lep_etcone20\", \"lep_ptcone30\", \"jet_n\", \"jet_pt\", \"jet_eta\", \"jet_phi\",\"jet_E\", \"jet_MV2c10\"], outputtype=tuple)\n",
    "    print(\"File has been successfully opened!\")\n",
    "    print(type(lep_pt))\n",
    "    print(lep_pt))\n",
    "\n",
    "    \n",
    "    leplv = LVepm.TLorentzVectorArray.from_ptetaphi(lep_pt, lep_eta, lep_phi, lep_E)\n",
    "\n",
    "    lep_reliso_pt = (lep_ptcone30 / lep_pt)\n",
    "    lep_reliso_et = (lep_etcone20 / lep_pt)\n",
    "    sum_lep_type = lep_type.sum()\n",
    "    \n",
    "    jetlv = LVepm.TLorentzVectorArray.from_ptetaphi(jet_pt, jet_eta, jet_phi, jet_E)\n",
    "    jetlv = jetlv[jet_MV2c10.argsort()]    \n",
    "    tags = jet_pt[jet_MV2c10 > btagWP77]\n",
    "\n",
    "    trig_cut = ( (trigM==1) | (trigE==1))\n",
    "    lep_kinematics_cut  = ( (lep_pt.max() > 20000) & (lep_pt.min() > 7000) & (lep_eta.min() >-2.5) & (lep_eta.max() < 2.5))\n",
    "    lep_type_cut  = ((sum_lep_type == 44) | (sum_lep_type == 48) | (sum_lep_type == 52))\n",
    "    lep_iso_cut =  ((lep_reliso_pt.max() < 0.3) & (lep_reliso_pt.max() < 0.3))\n",
    "    lept_count_cut = (leplv.counts ==4)\n",
    "    lept_charge_cut = (lep_charge.sum()==0)\n",
    "    \n",
    "    # filtering events according to cuts above that select out interesting events\n",
    "    event_cut = ( lep_kinematics_cut & lep_type_cut & lep_iso_cut  & lept_count_cut & lept_charge_cut)\n",
    "\n",
    "    first_lep_p4 =  leplv[event_cut,0]\n",
    "    second_lep_p4 = leplv[event_cut,1]\n",
    "    third_lep_p4 =  leplv[event_cut,2]\n",
    "    fourth_lep_p4 = leplv[event_cut,3]\n",
    "    mcWeight = mcWeight[event_cut] \n",
    "    \n",
    "    scaleFactor_PILEUP = scaleFactor_PILEUP[event_cut] \n",
    "    scaleFactor_ELE = scaleFactor_ELE[event_cut] \n",
    "    scaleFactor_MUON = scaleFactor_MUON[event_cut] \n",
    "    scaleFactor_LepTRIGGER = scaleFactor_LepTRIGGER[event_cut] \n",
    "    \n",
    "    #construct 4-lepton system by adding 4-vectors of the leptons vectorially\n",
    "    llll_p4 = first_lep_p4 + second_lep_p4 + third_lep_p4 + fourth_lep_p4  \n",
    "    \n",
    "    \n",
    "    # make histograms of the m_llll distribution for simulation and data\n",
    "    if(file.split(\"_\")[0] == \"mc\"):\n",
    "        finalWeight = get_xsec_weight(sample_name)*(mcWeight)*(scaleFactor_PILEUP)*(scaleFactor_ELE) *(scaleFactor_MUON)*(scaleFactor_LepTRIGGER)\n",
    "        H, b = np.histogram(llll_p4.mass/1000.0, weights=finalWeight, bins=bins_ar)                        \n",
    "        mc_hist_list.append(H)\n",
    "        if(\"H125\" in file):\n",
    "            H_125 = np.add(H, H_125)\n",
    "        else:\n",
    "            #print(\"Sample Name = \" + str(file) + \" exp. num. events = \" + str(np.sum(finalWeight)) )\n",
    "            H_bkg = np.add(H, H_bkg)\n",
    "    \n",
    "    else:\n",
    "        finalWeight = np.ones(len(mcWeight)) \n",
    "        sample_name = \"Data (10 fb^-1)\"\n",
    "        H_data, b = np.histogram(llll_p4.mass/1000.0, weights=finalWeight, bins=bins_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our events selected, we can compare our data with our expectations from theory. Our best theory of particle physics is known as the Standard Model, so the predictions here represent the expectations from the Standard Model after a simulation of how the ATLAS detector detects particles has been applied. \n",
    "\n",
    "Specifically, we plot a histogram of the number of collisions (\"events\") as a function of the mass of the four-lepton system. We use this mass because it distinguishes between the the Higgs signal, which has a peak at the higgs mass (125 GeV) and is zero elesehere, and the backgrounds which have a peak around 90 GeV and are flatter elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([H_bkg, H_125], bins=bins_ar, stack=True, label=[\"Backgrounds\", \"Higgs\"], histtype='fill')\n",
    "hep.histplot([H_data], bins=bins_ar, stack=False, yerr=True, histtype=\"errorbar\", color=\"black\",label=\"ATLAS Open Data\")\n",
    "\n",
    "plt.legend(loc=1, ncol=3, fontsize=9)\n",
    "plt.xlabel(\"m_llll [GeV]\")\n",
    "plt.ylabel(\"# events\")\n",
    "plt.ylim([0.0, 60])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 \n",
    " Discuss if the model(coloured histograms) by the is a good model for the data (black points) and why.\n",
    " Calculation of statistics to make your answer more quantitative is encouraged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a chi-squared function that will quantify how closely the model and data agree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcChiSq(obs, preds):\n",
    "    chiSq = 0.0\n",
    "    ndf = len(obs)\n",
    "    for bin in range(0, len(obs)):\n",
    "        diff = preds[bin] - obs[bin]\n",
    "        var = ( np.abs(preds[bin])) \n",
    "        if (var != 0):\n",
    "            chiSq += (diff**2)/(var)\n",
    "            #print(\"obs, pred, diff, var  chi contrib = \"+  str( obs[bin]) +\" \" +  str(preds[bin]) +\" \"+  str(diff) +\" \"+  str(var) +\" \" + str((diff**2)/(var)))\n",
    "    return chiSq, ndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting s_s\n",
    "  We can make the crude assumption that the only thing wrong with our model is that the predicted total number of events from the Higgs signal is wrong by some single multiplicative factor which we will call s_s. We can investigate then how the agreement between the model and the data could be improve by applying various values of s_s and observing the change in the chi-squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_s_ar = np.linspace(1.0, 3.0, 100) # array of s_s values we will investigate\n",
    "\n",
    "chi2_ar = np.empty( len(s_s_ar) ) # empty array of that will hold the chi2 values we will calculate\n",
    "\n",
    "minChi2 = 1000000\n",
    "bestFit_s_s = 0.0 #starting values for min chi2 and best value of s_s\n",
    "\n",
    "for s_s in range(0, len(s_s_ar)): #looping over out s_s values \n",
    "    pred = (s_s_ar[s_s]*H_125) + (H_bkg) # generating a prediction according to this s_s\n",
    "    chi2 = calcChiSq(H_data, pred)[0] # calculating chi2\n",
    "    chi2_ar[s_s] = chi2 # adding chi2 value to chi2 array \n",
    "    if(chi2 < minChi2): # check if this is the lowest chi2 we have seen so far\n",
    "         minChi2 = chi2 # update lowest chi2 value seen \n",
    "         bestFit_s_s = s_s_ar[s_s] # update value for best fit s_s\n",
    "\n",
    "deltaChi2_ar = chi2_ar - minChi2 # make array of delta chi2 values\n",
    "\n",
    "#we expect the chi2 vs. mZ curve to be quadratic, so let's fit that function to it.\n",
    "z = np.polyfit(s_s_ar, deltaChi2_ar, 2) #\"2\" for a second-order polynomial\n",
    "p = np.poly1d(z)\n",
    "\n",
    "# we can display the estimated uncertianty on mZ via critical values of the delta chi-squared curve\n",
    "y0 = 1.0\n",
    "crit = (p - y0).roots # roots of the polynominal -1, i.e., the mz values where p = 1 \n",
    "\n",
    "#shading in the confience interval band \n",
    "px=np.arange(crit[1],crit[0],0.001)\n",
    "        \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(s_s_ar, deltaChi2_ar, 'k', linewidth=2)\n",
    "ax.fill_between(px,p(px),alpha=0.5, color='g', label=\"uncertainty\")\n",
    "\n",
    "plt.xlabel(\"s_s\")\n",
    "plt.ylabel(\"delta chi-squared\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Best fit value of s_s = \" + str(round(bestFit_s_s, 3)) + \" +/- \" + str(round(np.abs(crit[0] - bestFit_s_s),2 )) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "  Discuss the results of the fitting of s_s. Is the fitted model now a successful mode for the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting s_b\n",
    "  Alternatively, we can make the crude assumption that the only thing wrong with our model is that the predicted total number of events from the *backgrounds* is wrong by some single multiplicative factor which we will call s_b.\n",
    " We can investigate then how the agreement between the model and the data could be improve by applying various values of s_s and observing the change in the chi-squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_b_ar = np.linspace(1.15, 1.4, 100)\n",
    "\n",
    "chi2_ar = np.empty( len(s_b_ar) )\n",
    "\n",
    "minChi2 = 1000000\n",
    "bestFit_s_b = 0.0\n",
    "\n",
    "for s_b in range(0, len(s_b_ar)):\n",
    "    pred = (H_125) + (s_b_ar[s_b]*H_bkg)\n",
    "    chi2 = calcChiSq(H_data, pred)[0]\n",
    "    chi2_ar[s_b] = chi2\n",
    "    if(chi2 < minChi2):\n",
    "        minChi2 = chi2\n",
    "        bestFit_s_b = s_b_ar[s_b]\n",
    "        \n",
    "deltaChi2_ar = chi2_ar - minChi2\n",
    "\n",
    "z = np.polyfit(s_b_ar, deltaChi2_ar, 2)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "y0 = 1.0 # this is the value of the delta chi-squared function that defines the 68% CI for a one parameter fit.\n",
    "         # we'll invetigate if this parmaeter makes sense in the final exercise.\n",
    "crit = (p - y0).roots \n",
    "\n",
    "px=np.arange(crit[1],crit[0],0.001)\n",
    "        \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(s_b_ar, deltaChi2_ar, 'k', linewidth=2)\n",
    "ax.fill_between(px,p(px),alpha=0.5, color='g', label=\"uncertainty\")\n",
    "plt.xlabel(\"s_b\")\n",
    "plt.ylabel(\"delta chi-squared\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Best fit value of s_b = \" + str(round(bestFit_s_b, 3)) + \" +/- \" + str(round(np.abs(crit[0] - bestFit_s_b),2 )) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "Discuss the results of the fitting of s_b. Is the fitted model now a successful mode for the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting s_s and s_b\n",
    "More realistically, we can assume that the the predicted total numbers of events from both the Higgs signal AND the backgrounds are wrong by some two seperate multiplicative factors. We can investigate then how the agreement between the model and the data could be improve by applying various values of (s_s, s_b) and observing the change in the chi-squared. We now need to plot the delta chi-squared as a 2-D function of (s_s, s_b) as it depends on both parmaters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_s_ar = np.linspace(0.1, 2.7, 100)\n",
    "s_b_ar = np.linspace(1.1, 1.5, 100)\n",
    "\n",
    "chi2_ar = np.empty( (len(s_s_ar), len(s_b_ar) ))\n",
    "\n",
    "bestFit_s_s = 0.0\n",
    "bestFit_s_b = 0.0\n",
    "\n",
    "minChi2 = 1000000\n",
    "\n",
    "for s_s in range(0, len(s_s_ar)):\n",
    "    for s_b in range(0, len(s_b_ar)):\n",
    "        pred = (s_s_ar[s_s]*H_125) + (s_b_ar[s_b]*H_bkg)\n",
    "        chi2 = calcChiSq(H_data, pred)[0]\n",
    "        chi2_ar[s_s, s_b] = chi2 \n",
    "        if(chi2 < minChi2):\n",
    "            minChi2 = chi2\n",
    "            bestFit_s_s = s_s_ar[s_s]\n",
    "            bestFit_s_b = s_b_ar[s_b]\n",
    "\n",
    "deltaChi2_ar = chi2_ar - minChi2\n",
    "\n",
    "levels = [2.3] # this is the value of the delta chi-squared function that defines the 68% CI for a two parameter fit.\n",
    "         # we'll invetigate if this parmaeter makes sense in the final exercise.\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.gca()\n",
    "\n",
    "cfset = plt.contourf(s_b_ar, s_s_ar, deltaChi2_ar,  cmap='coolwarm')\n",
    "cset = plt.contour(s_b_ar, s_s_ar, deltaChi2_ar, levels=levels, colors=['white'])\n",
    "\n",
    "bf_point = plt.scatter(bestFit_s_b, bestFit_s_s, color='black')\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(cfset)\n",
    "cbar.set_label('Delta Chi-squared', fontsize=15, rotation=270)\n",
    "ax.clabel(cset, inline=1, fontsize=10)\n",
    "ax.set_xlabel('s_b',fontsize=18)\n",
    "ax.set_ylabel('s_s',fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "#extract 2D result\n",
    "print(\"best-fit s_s = \" + str(bestFit_s_s))\n",
    "print(\"best-fit s_b = \" + str(bestFit_s_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "Discuss the results of the simulatenous fit of s_s and s_b. How do the results of the three fitting approaches compare? If we interpret the Confidence Intervals on s_s and s_b as uncertainties on measurements of these parameters, then how do the uncertainties compare? Have we measured one parameter better than the other? If so, why? Does it make sense given the histograms we used to perform the fit of these parameters? How do the best-fit vales of the s_s and s_b comapre to the values when we fit one parameter at a time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a 3D version of the 2-D contour Confidence Interval plot we made earlier, for no reason other than it looks cool and making cool plots is one of the main reasons data analysis is fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(s_b_ar, s_s_ar, deltaChi2_ar, 80, cmap='coolwarm')\n",
    "ax.set_xlabel('s_b')\n",
    "ax.set_ylabel('s_s')\n",
    "ax.set_zlabel('delta chi-squared')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have estimates of the 'best-fit' values of s_s and s_b. Let's apply these factors to the predictions for signal and background and see how the updated predctions compare to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "\n",
    "H_125_bf = bestFit_s_s*H_125\n",
    "H_bkg_bf = bestFit_s_b*H_bkg\n",
    "\n",
    "hep.histplot([H_bkg_bf, H_125_bf], bins=bins_ar, stack=True, label=[\"Backgrounds (best-fit)\", \"Higgs (best-fit)\"], histtype='fill')\n",
    "hep.histplot([H_data], bins=bins_ar, stack=False, yerr=True, histtype=\"errorbar\", color=\"black\",label=\"ATLAS Open Data\")\n",
    "plt.legend(loc=1, ncol=2, fontsize=9)\n",
    "plt.xlabel(\"m_llll [GeV]\")\n",
    "plt.ylabel(\"# events\")\n",
    "plt.ylim([0.0, 60])\n",
    "plt.show()\n",
    "\n",
    "print(\"N Higgs events = \" + str(np.sum(H_125_bf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data and our best-fit predictions agree very well... right?.......right??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goodness of fit\n",
    "We see that our fit succeeded it produced predictions for our signal and background that seem to agree much more closely with data than the original predictions. However, are we sure that our fitted model is a 'good' model for the data? We could as the question: \"If this is the correct model, then how probable is our observed data?\". This is answered by working our the p-value of our observed data acording to our fitted model. \n",
    "\n",
    "In the cell below, we take out fitted model and assume each bin is distrubuted as a gaussian with mean equal to the prediction in the bin and standard deviation equal to the square root of the mean. We generate N toy from the this fitted model and calculate the chi-squared for each toy with respect to the fitted model.\n",
    "\n",
    "Overlaying the chi-squared value of our ATLAS data with respect to our model \"If this is the correct model, then how probable is our observed data?\". This is answered by working our the p-value of our observed data acording to our fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the chi-2 distribution of N toy experiments based on the fitted model \n",
    "# assume a gaussian-distributed bin height with mean = model pred. and sigma equal root model pred\n",
    "# what then is the p-value of the data with respect to the distribution?\n",
    "    \n",
    "def generate_toy(means): #function to generate a toy histogram given the mean for each bin\n",
    "        toy = np.empty(len(means))\n",
    "        for i in range(0, len(means)):\n",
    "            toy_bin = np.random.normal(means[i], np.sqrt(means[i]), 1)\n",
    "            toy[i] = toy_bin\n",
    "        #print(\"toy = \" + str(toy))\n",
    "        return toy\n",
    "\n",
    "ntoys = 10000 # start with a small number, increase when you undertsand your results\n",
    "\n",
    "means = H_bkg_bf + H_125_bf # take the results of the fit as the means of the fitted model\n",
    "\n",
    "#print(\"means\" + str(means))\n",
    "\n",
    "chi2_toys = np.empty(ntoys)\n",
    "\n",
    "for t in range(0, ntoys):\n",
    "    toy = generate_toy(means) \n",
    "    chi2_toys[t] = calcChiSq(toy, means)[0]\n",
    "    #print(\"chi2 \" + str(chi2_toys[t]))\n",
    "    \n",
    "plt.figure()\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "#plot distribtion of chi-squared values from toys\n",
    "bins_ar = np.linspace(0.0, 200, num=(nBins+1))\n",
    "chi2Hist, chi2bins = np.histogram(chi2_toys, bins=bins_ar, density=True)                        \n",
    "hep.histplot([chi2Hist], bins=chi2bins, histtype='fill', label=\"toys\")\n",
    "\n",
    "#plot expected distrutuon of chi-squared values from theory - chi-squared distribtuions with ndof = nbins -2\n",
    "df = nBins - 2\n",
    "x = np.linspace(stats.chi2.ppf(0.000001, df), stats.chi2.ppf(0.999999, df), 200)\n",
    "label = '$\\chi^{2}$ (ndof = ' + str(df) + ') pdf'\n",
    "ax1.plot(x, stats.chi2.pdf(x, df), 'r-', lw=5, alpha=0.6, label=label)\n",
    "\n",
    "# overlay the chi2 squared value from the ATLAS OpenData\n",
    "plt.plot(chi2_data,0.0 , 'g*', ms=14, label='ATLAS OpenData')\n",
    "plt.legend()\n",
    "plt.xlabel(\"chi^2\")\n",
    "plt.ylabel(\"# toys\")\n",
    "\n",
    "print(\"chi2 value of ATLAS Open data is = \" + str(chi2_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Goodness of fit and p-value\n",
    "Calculate the p-value of the ATLAS data with respect to the chi-squared distribution shown above. Comment if this value indicates if the fitted model is a good model for the ATLAS data or not. Repeat the procedure to calculate the p-value of the data with respect to the model **before** any fitting occured. How do the two p-values compare? What do you conclude about the fitted model? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exercise 6 - How do we know the critical value for a Confidence Interval?\n",
    "Earlier we used critical value of 2.3 of the delta chi-squared function to define the 68% Confidence Interval on our fit parameters. This value of 2.3 is based on the propertires of the Gaussian distribution. In this exercise we aim to check that this value of 2.3 indeed gives us the correct 68 % CL. Recall the definition of a X % CL: *an interval constructed such that if we constructed this interval in the same way for a large number of repeated experiments, it would contain the true value of the parameter in X % of those experiments*.\n",
    "\n",
    "Use random numbers to work out the critical value of the delta chi-squared corresponding to the boundaries of the 68 % Confidence Interval for the fit performed above. Some hints are given towards the steps needed in the cell below. The steps are not neccessarily in the correct order, they rather they repreent the crucial pieces of information you need to build a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggested steps\n",
    "\n",
    "    # generate N toys\n",
    "\n",
    "        # for each toy, run 2-parameter fit and estimate CL based on critical value d\n",
    "        # (note that you will fit the toy to the 'true' distributions, not real data)\n",
    "\n",
    "        # check if true value is within the contour of the delta chiquared function defined by d\n",
    "\n",
    "    # calculate fraction of toys (f) in which the CL estimation for critical value\n",
    "    # contains true values of s_s and s_b\n",
    "\n",
    "    # if you can check the fraction of toys with CI containing the true value \n",
    "    # or a given d, you just need to repeat this  for a range of d values.\n",
    "    # this will take some time, so start with a small number of toys and a few \n",
    "    # d values, when you understand your results you can increse both.\n",
    "\n",
    "    # plot f as a function of d\n",
    "\n",
    "    # read form this plot the critical value that gives you the 68% CI.\n",
    "    \n",
    "# Best of luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
